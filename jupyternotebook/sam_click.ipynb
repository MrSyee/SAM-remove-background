{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example code of Segment Anything Model (SAM)\n",
    "\n",
    "Colab 환경에서 SAM 모델을 사용해 이미지에 클릭한 위치의 객체를 segmentation 하는 예제입니다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Colab 환경 설정\n",
    "예제를 실행시키기 위해 python package들을 설치합니다. 예제로 사용할 이미지들도 다운로드 받습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local에서 Run하는 경우 False로 변경\n",
    "using_colab = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if using_colab:\n",
    "    !wget https://raw.githubusercontent.com/mrsyee/sam-remove-background/main/jupyternotebook/requirements.txt\n",
    "    !pip install -r requirements.txt\n",
    "\n",
    "    # Download examples\n",
    "    !mkdir examples\n",
    "    !cd examples && wget https://raw.githubusercontent.com/mrsyee/sam-remove-background/main/assets/examples/mannequin.jpeg"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib\n",
    "from typing import Tuple\n",
    "\n",
    "import cv2\n",
    "import gradio as gr\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "from segment_anything import SamPredictor, sam_model_registry"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "CHECKPOINT_PATH = os.path.join(\"checkpoint\")\n",
    "CHECKPOINT_NAME = \"sam_vit_h_4b8939.pth\"\n",
    "CHECKPOINT_URL = \"https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth\"\n",
    "MODEL_TYPE = \"default\"\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(DEVICE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize and load pre-trained SAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(CHECKPOINT_PATH):\n",
    "    os.makedirs(CHECKPOINT_PATH, exist_ok=True)\n",
    "checkpoint = os.path.join(CHECKPOINT_PATH, CHECKPOINT_NAME)\n",
    "if not os.path.exists(checkpoint):\n",
    "    urllib.request.urlretrieve(CHECKPOINT_URL, checkpoint)\n",
    "sam = sam_model_registry[MODEL_TYPE](checkpoint=checkpoint).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = SamPredictor(sam)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segment with one click"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_masks(\n",
    "    masks: np.ndarray, iou_preds: np.ndarray, num_points: int\n",
    ") -> Tuple [np.ndarray, np.ndarray]:\n",
    "    # Determine if we should return the multiclick mask or not from the number of points.\n",
    "    # The reweighting is used to avoid control flow.\n",
    "    # Reference: https://github.com/facebookresearch/segment-anything/blob/6fdee8f2727f4506cfbbe553e23b895e27956588/segment_anything/utils/onnx.py#L92-L105\n",
    "    score_reweight = np.array([1000] + [0] * 2)\n",
    "    score = iou_preds + (num_points - 2.5) * score_reweight\n",
    "    best_idx = np.argmax(score)\n",
    "    masks = np.expand_dims(masks[best_idx, :, :], axis=-1)\n",
    "    iou_preds = np.expand_dims(iou_preds[best_idx], axis=0)\n",
    "    return masks, iou_preds\n",
    "\n",
    "\n",
    "def segment(image: np.ndarray, point_w: int, point_h: int) -> np.ndarray:\n",
    "    points_coords = np.array([[point_w, point_h], [0, 0]])\n",
    "    points_label = np.array([1, -1])\n",
    "\n",
    "    # Preprocess image and get image embedding with SAM Encoder\n",
    "    predictor.set_image(image)\n",
    "\n",
    "    # Inference SAM Decoder model with point information.\n",
    "    masks, scores, _ = predictor.predict(points_coords, points_label)\n",
    "\n",
    "    # Select the best mask based on the score.\n",
    "    mask, _ = select_masks(masks, scores, points_coords.shape[0])\n",
    "    mask = (mask > 0).astype(np.uint8) * 255\n",
    "    mask = cv2.cvtColor(mask, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "    return mask"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UI: Upload image and click"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coords(evt: gr.SelectData):\n",
    "    return evt.index[0], evt.index[1]\n",
    "\n",
    "def segment_by_click(image: np.ndarray, evt: gr.SelectData):\n",
    "    click_w, click_h = evt.index\n",
    "    return segment(image, click_w, click_h)\n",
    "\n",
    "with gr.Blocks() as app:\n",
    "    gr.Markdown(\"# Example of SAM with 1 click\")\n",
    "    with gr.Row():\n",
    "        coord_w = gr.Number(label=\"Mouse coords w\")\n",
    "        coord_h = gr.Number(label=\"Mouse coords h\")\n",
    "\n",
    "    with gr.Row():\n",
    "        input_img = gr.Image(label=\"Input image\").style(height=600)\n",
    "        output_img = gr.Image(label=\"Output image\").style(height=600)\n",
    "\n",
    "    input_img.select(get_coords, None, [coord_w, coord_h])\n",
    "    input_img.select(segment_by_click, [input_img], output_img)\n",
    "\n",
    "    gr.Markdown(\"## Image Examples\")\n",
    "    gr.Examples(\n",
    "        examples=[\n",
    "            [\"examples/mannequin.jpeg\", 1720, 230]\n",
    "        ],\n",
    "        inputs=[input_img, coord_h, coord_w],\n",
    "        outputs=output_img,\n",
    "        fn=segment,\n",
    "        run_on_click=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app.launch(inline=False, share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7860\n"
     ]
    }
   ],
   "source": [
    "app.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sam-remove-background",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
