{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example code of Segment Anything Model (SAM)\n",
    "\n",
    "Colab 환경에서 SAM 모델을 사용해 이미지에 클릭한 위치의 객체를 segmentation 하는 예제입니다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Colab 환경 설정\n",
    "예제를 실행시키기 위해 python package들을 설치합니다. 예제로 사용할 이미지들도 다운로드 받습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local에서 Run하는 경우 False로 변경\n",
    "using_colab = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if using_colab:\n",
    "    !wget https://raw.githubusercontent.com/mrsyee/sam-remove-background/main/jupyternotebook/requirements.txt\n",
    "    !pip install -r requirements.txt\n",
    "\n",
    "    # Download examples\n",
    "    !mkdir examples\n",
    "    !cd examples && wget https://raw.githubusercontent.com/mrsyee/sam-remove-background/main/assets/examples/mannequin.jpeg"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib\n",
    "from typing import Tuple\n",
    "\n",
    "import cv2\n",
    "import gradio as gr\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "from segment_anything import SamPredictor, sam_model_registry"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_PATH = os.path.join(\"checkpoint\")\n",
    "CHECKPOINT_NAME = \"sam_vit_h_4b8939.pth\"\n",
    "CHECKPOINT_URL = \"https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth\"\n",
    "MODEL_TYPE = \"default\"\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(DEVICE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize and load pre-trained SAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(CHECKPOINT_PATH):\n",
    "    os.makedirs(CHECKPOINT_PATH, exist_ok=True)\n",
    "checkpoint = os.path.join(CHECKPOINT_PATH, CHECKPOINT_NAME)\n",
    "if not os.path.exists(checkpoint):\n",
    "    urllib.request.urlretrieve(CHECKPOINT_URL, checkpoint)\n",
    "sam = sam_model_registry[MODEL_TYPE](checkpoint=checkpoint).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = SamPredictor(sam)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segment with one click"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_masks(\n",
    "    masks: np.ndarray, iou_preds: np.ndarray, num_points: int\n",
    ") -> Tuple [np.ndarray, np.ndarray]:\n",
    "    # Determine if we should return the multiclick mask or not from the number of points.\n",
    "    # The reweighting is used to avoid control flow.\n",
    "    # Reference: https://github.com/facebookresearch/segment-anything/blob/6fdee8f2727f4506cfbbe553e23b895e27956588/segment_anything/utils/onnx.py#L92-L105\n",
    "    score_reweight = np.array([1000] + [0] * 2)\n",
    "    score = iou_preds + (num_points - 2.5) * score_reweight\n",
    "    best_idx = np.argmax(score)\n",
    "    masks = np.expand_dims(masks[best_idx, :, :], axis=-1)\n",
    "    iou_preds = np.expand_dims(iou_preds[best_idx], axis=0)\n",
    "    return masks, iou_preds\n",
    "\n",
    "\n",
    "def segment(\n",
    "        image: np.ndarray, point_h: int, point_w: int, point_label: int\n",
    "    ) -> np.ndarray:\n",
    "    print(\"[INFO] Segment.\")\n",
    "    points_coords = np.array([[point_h, point_w], [0, 0]])\n",
    "    points_label = np.array([point_label, -1])\n",
    "\n",
    "    # Preprocess image and get image embedding with SAM Encoder\n",
    "    print(\"[INFO] Preprocess image and get image embedding.\")\n",
    "    predictor.set_image(image)\n",
    "\n",
    "    # Inference SAM Decoder model with point information.\n",
    "    print(\"[INFO] Get mask.\")\n",
    "    masks, scores, _ = predictor.predict(points_coords, points_label)\n",
    "\n",
    "    # Select the best mask based on the score.\n",
    "    mask, _ = select_masks(masks, scores, points_coords.shape[0])\n",
    "    mask = mask.astype(np.uint8) * 255\n",
    "    print(mask.shape)\n",
    "\n",
    "    return mask"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UI: Upload image and click"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coords(evt: gr.SelectData):\n",
    "    return evt.index[0], evt.index[1]\n",
    "\n",
    "def segment_by_click(image: np.ndarray, evt: gr.SelectData):\n",
    "    click_h, click_w = evt.index\n",
    "    return segment(image, click_h, click_w, 1)\n",
    "\n",
    "with gr.Blocks() as app:\n",
    "    gr.Markdown(\"# Example of SAM with 1 click\")\n",
    "    with gr.Row():\n",
    "        coord_h = gr.Number(label=\"Mouse coords h\")\n",
    "        coord_w = gr.Number(label=\"Mouse coords w\")\n",
    "\n",
    "    with gr.Row():\n",
    "        input_img = gr.Image(label=\"Input image\").style(height=600)\n",
    "        output_img = gr.Image(label=\"Output image\").style(height=600)\n",
    "\n",
    "    input_img.select(get_coords, None, [coord_h, coord_w])\n",
    "    input_img.select(segment_by_click, [input_img], output_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7863\n",
      "Running on public URL: https://75483381dde447ac18.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades (NEW!), check out Spaces: https://huggingface.co/spaces\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Segment.\n",
      "[INFO] Preprocess image and get image embedding.\n",
      "[INFO] Get mask.\n",
      "(2005, 2769, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/kyunghwankim/opt/anaconda3/envs/sam-remove-background/lib/python3.9/site-packages/PIL/Image.py\", line 3089, in fromarray\n",
      "    mode, rawmode = _fromarray_typemap[typekey]\n",
      "KeyError: ((1, 1, 1), '|u1')\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/kyunghwankim/opt/anaconda3/envs/sam-remove-background/lib/python3.9/site-packages/gradio/routes.py\", line 422, in run_predict\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"/Users/kyunghwankim/opt/anaconda3/envs/sam-remove-background/lib/python3.9/site-packages/gradio/blocks.py\", line 1326, in process_api\n",
      "    data = self.postprocess_data(fn_index, result[\"prediction\"], state)\n",
      "  File \"/Users/kyunghwankim/opt/anaconda3/envs/sam-remove-background/lib/python3.9/site-packages/gradio/blocks.py\", line 1260, in postprocess_data\n",
      "    prediction_value = block.postprocess(prediction_value)\n",
      "  File \"/Users/kyunghwankim/opt/anaconda3/envs/sam-remove-background/lib/python3.9/site-packages/gradio/components.py\", line 1862, in postprocess\n",
      "    return processing_utils.encode_array_to_base64(y)\n",
      "  File \"/Users/kyunghwankim/opt/anaconda3/envs/sam-remove-background/lib/python3.9/site-packages/gradio/processing_utils.py\", line 92, in encode_array_to_base64\n",
      "    pil_image = Image.fromarray(_convert(image_array, np.uint8, force_copy=False))\n",
      "  File \"/Users/kyunghwankim/opt/anaconda3/envs/sam-remove-background/lib/python3.9/site-packages/PIL/Image.py\", line 3092, in fromarray\n",
      "    raise TypeError(msg) from e\n",
      "TypeError: Cannot handle this data type: (1, 1, 1), |u1\n"
     ]
    }
   ],
   "source": [
    "app.launch(inline=False, share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7863\n"
     ]
    }
   ],
   "source": [
    "app.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sam-remove-background",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
