{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example code of Segment Anything Model (SAM)\n",
    "\n",
    "Colab 환경에서 SAM 모델을 사용해 이미지에 클릭한 위치의 객체를 segmentation 하는 예제입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\n",
    "\"\"\"\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/MrSyee/SAM-remove-background/blob/main/jupyternotebook/sam_click.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\"\"\"\n",
    "))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Colab 환경 설정\n",
    "예제를 실행시키기 위해 python package들을 설치합니다. 예제로 사용할 이미지들도 다운로드 받습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local에서 Run하는 경우 False로 변경\n",
    "using_colab = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if using_colab:\n",
    "    !wget https://raw.githubusercontent.com/mrsyee/sam-remove-background/main/jupyternotebook/requirements.txt\n",
    "    !pip install -r requirements.txt\n",
    "\n",
    "    # Download examples\n",
    "    !mkdir examples\n",
    "    !cd examples && wget https://raw.githubusercontent.com/mrsyee/sam-remove-background/main/assets/examples/mannequin.jpeg"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib\n",
    "from typing import Tuple\n",
    "\n",
    "import cv2\n",
    "import gradio as gr\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from segment_anything import SamPredictor, sam_model_registry"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_PATH = os.path.join(\"checkpoint\")\n",
    "CHECKPOINT_NAME = \"sam_vit_h_4b8939.pth\"\n",
    "CHECKPOINT_URL = \"https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth\"\n",
    "MODEL_TYPE = \"default\"\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(DEVICE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize and load pre-trained SAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(CHECKPOINT_PATH):\n",
    "    os.makedirs(CHECKPOINT_PATH, exist_ok=True)\n",
    "checkpoint = os.path.join(CHECKPOINT_PATH, CHECKPOINT_NAME)\n",
    "if not os.path.exists(checkpoint):\n",
    "    urllib.request.urlretrieve(CHECKPOINT_URL, checkpoint)\n",
    "sam = sam_model_registry[MODEL_TYPE](checkpoint=checkpoint).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = SamPredictor(sam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 예제 이미지 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread(\"examples/mannequin.jpeg\", cv2.IMREAD_COLOR)\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(image)\n",
    "plt.axis(\"on\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Point 입력하기\n",
    "원하는 위치의 객체를 segmentation 하기 위해서는 원하는 위치를 Point로 입력해야합니다. SAM 모델에 Point를 입력하기 위해서는 두 가지 정보가 필요합니다.\n",
    "\n",
    "- point_coords: Point 좌표 (x, y)\n",
    "- points_labels: Point의 타입. Point 위치의 객체를 선택하려면 Positive Click(1)로 설정."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "point_coords = np.array([[1720, 230]])\n",
    "points_labels = np.array([1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(image)\n",
    "plt.gca().scatter(\n",
    "    point_coords[0, 0],\n",
    "    point_coords[0, 1],\n",
    "    color=\"green\",\n",
    "    marker=\"o\",\n",
    "    s=200,\n",
    "    edgecolor=\"white\",\n",
    "    linewidth=1.25,\n",
    ")\n",
    "plt.axis(\"on\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SAM 모델 추론\n",
    "predictor의 predict 함수를 통해 추론합니다. 추론의 결과로 2 가지 정보가 출력됩니다.\n",
    "\n",
    "- masks: 입력한 정보에 대해 3개의 mask가 출력.\n",
    "- scores: 3개의 mask의 퀄리티에 대한 모델의 평가 점수. 점수가 가장 높을수록 퀄리티가 높은 mask입니다.\n",
    "- low_res_logits: 저해상도 mask 출력. 이 예제에서는 사용하지 않습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.set_image(image)\n",
    "masks, scores, _ = predictor.predict(point_coords, points_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, mask in enumerate(masks):\n",
    "    print(f\"Mask {i}\")\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(mask)\n",
    "    plt.axis(\"on\")\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segment with one click"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_masks(\n",
    "    masks: np.ndarray, iou_preds: np.ndarray, num_points: int\n",
    ") -> Tuple [np.ndarray, np.ndarray]:\n",
    "    # Determine if we should return the multiclick mask or not from the number of points.\n",
    "    # The reweighting is used to avoid control flow.\n",
    "    # Reference: https://github.com/facebookresearch/segment-anything/blob/6fdee8f2727f4506cfbbe553e23b895e27956588/segment_anything/utils/onnx.py#L92-L105\n",
    "    score_reweight = np.array([1000] + [0] * 2)\n",
    "    score = iou_preds + (num_points - 2.5) * score_reweight\n",
    "    best_idx = np.argmax(score)\n",
    "    masks = np.expand_dims(masks[best_idx, :, :], axis=-1)\n",
    "    iou_preds = np.expand_dims(iou_preds[best_idx], axis=0)\n",
    "    return masks, iou_preds\n",
    "\n",
    "\n",
    "def segment(image: np.ndarray, point_x: int, point_y: int) -> np.ndarray:\n",
    "    points_coords = np.array([[point_x, point_y], [0, 0]])\n",
    "    points_label = np.array([1, -1])\n",
    "\n",
    "    # Inference SAM Decoder model with point information.\n",
    "    masks, scores, _ = predictor.predict(points_coords, points_label)\n",
    "\n",
    "    # Select the best mask based on the score.\n",
    "    mask, _ = select_masks(masks, scores, points_coords.shape[0])\n",
    "    mask = (mask > 0).astype(np.uint8) * 255\n",
    "    mask = cv2.cvtColor(mask, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "    return mask"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UI: Upload image and click"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_image(image: np.ndarray):\n",
    "    # Preprocess image and get image embedding with SAM Encoder\n",
    "    predictor.set_image(image)\n",
    "\n",
    "def get_coords(evt: gr.SelectData):\n",
    "    return evt.index[0], evt.index[1]\n",
    "\n",
    "def segment_by_click(image: np.ndarray, evt: gr.SelectData):\n",
    "    click_w, click_h = evt.index\n",
    "    return segment(image, click_w, click_h)\n",
    "\n",
    "with gr.Blocks() as app:\n",
    "    gr.Markdown(\"# Example of SAM with 1 click\")\n",
    "    with gr.Row():\n",
    "        coord_w = gr.Number(label=\"Mouse coords w\")\n",
    "        coord_h = gr.Number(label=\"Mouse coords h\")\n",
    "\n",
    "    with gr.Row():\n",
    "        input_img = gr.Image(label=\"Input image\").style(height=600)\n",
    "        output_img = gr.Image(label=\"Output image\").style(height=600)\n",
    "    \n",
    "    input_img.upload(set_image, [input_img], None)\n",
    "    input_img.select(get_coords, None, [coord_w, coord_h])\n",
    "    input_img.select(segment_by_click, [input_img], output_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app.launch(inline=False, share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sam-remove-background",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
